{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWeSIJVqNdq5"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLCs0we2bHGg"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install decord\n",
        "!pip install git+https://github.com/facebookresearch/sam3.git"
      ],
      "metadata": {
        "id": "bNjFWorfPQka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import spacy\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from sam3.model_builder import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from sam3.visualization_utils import (\n",
        "    load_frame,\n",
        "    prepare_masks_for_visualization,\n",
        "    visualize_formatted_frame_output,\n",
        ")\n",
        "\n",
        "import sam3"
      ],
      "metadata": {
        "id": "sBhmB1PbNchR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_gif(images, path=\"temp.gif\"):\n",
        "  # Render the images as the gif (15Hz control frequency):\n",
        "  images[0].save(path, save_all=True, append_images=images[1:], duration=int(1000/15), loop=0)\n",
        "  gif_bytes = open(path,\"rb\").read()\n",
        "  return gif_bytes"
      ],
      "metadata": {
        "id": "VnPpceOQRZ0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu4NdR8Q-rWC"
      },
      "outputs": [],
      "source": [
        "#OPTIONAL: Save the images\n",
        "\n",
        "OUTPUT_DIR = \"/content/droid_frames_exterior_2_left\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "ds = tfds.load(\n",
        "    \"droid_100\",\n",
        "    data_dir=\"gs://gresearch/robotics\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "for episode in ds.shuffle(10, seed=6).take(1):\n",
        "    for step in tqdm(episode[\"steps\"]):\n",
        "        frame = step[\"observation\"][\"exterior_image_1_left\"].numpy()\n",
        "\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(\n",
        "            os.path.join(OUTPUT_DIR, f\"{frame_idx:05d}.jpg\"),\n",
        "            quality=95,\n",
        "            subsampling=0\n",
        "        )\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "print(f\"Saved {frame_idx} frames to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Spacy for objects extraction from instruction**"
      ],
      "metadata": {
        "id": "a6S1f9uLOI9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SYvxkkyiwdM"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qHqBLr3pZy"
      },
      "outputs": [],
      "source": [
        "def extract_objects(instruction):\n",
        "    doc = nlp(instruction.lower())\n",
        "    objects = []\n",
        "\n",
        "    for chunk in doc.noun_chunks:\n",
        "\n",
        "        token_text = chunk.text.strip()\n",
        "\n",
        "        if token_text in [\"a\", \"the\", \"it\", \"they\", \"them\", \"this\", \"that\"]:\n",
        "            continue\n",
        "\n",
        "        if token_text not in objects:\n",
        "            objects.append(token_text)\n",
        "\n",
        "    objects = [obj for obj in objects if obj.isalpha() or \" \" in obj]\n",
        "\n",
        "    return objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCRoqMAtBeGe"
      },
      "source": [
        "**Sam3 only Method (Prompt: text)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGJJoNn0ow4q"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = build_sam3_image_model()\n",
        "processor = Sam3Processor(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_WRBqAh5O-o"
      },
      "outputs": [],
      "source": [
        "ds = tfds.load(\"droid_100\", data_dir=\"gs://gresearch/robotics\", split=\"train\")\n",
        "SEED = 3\n",
        "# 6\n",
        "\n",
        "for episode in ds.shuffle(10, seed=SEED).take(1):\n",
        "  # print(episode)\n",
        "  steps = episode[\"steps\"]\n",
        "  first_step = next(iter(steps))\n",
        "  instruction = first_step[\"language_instruction\"].numpy().decode(\"utf-8\")\n",
        "\n",
        "original_images = []\n",
        "for episode in ds.shuffle(10, seed=SEED).take(1):\n",
        "  for i, step in enumerate(episode[\"steps\"]):\n",
        "    original_images.append(\n",
        "      Image.fromarray(\n",
        "        np.concatenate((\n",
        "              step[\"observation\"][\"exterior_image_1_left\"].numpy(),\n",
        "              step[\"observation\"][\"exterior_image_2_left\"].numpy(),\n",
        "              step[\"observation\"][\"wrist_image_left\"].numpy(),\n",
        "        ), axis=1)\n",
        "      )\n",
        "    )\n",
        "\n",
        "text_labels = extract_objects(instruction)\n",
        "print(\"Instruction: \", instruction)\n",
        "print(\"Objects: \", text_labels)\n",
        "display.Image(as_gif(original_images))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in ds.shuffle(10, seed=SEED).take(1):\n",
        "  # print(episode)\n",
        "  steps = episode[\"steps\"]\n",
        "  first_step = next(iter(steps))\n",
        "  instruction = first_step[\"language_instruction_2\"].numpy().decode(\"utf-8\")\n",
        "  print(instruction)"
      ],
      "metadata": {
        "id": "a53xQTI9wnsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw5wUpiOt8Gk"
      },
      "outputs": [],
      "source": [
        "def main_pipeline(ori_images, instr):\n",
        "\n",
        "  masked_images = []\n",
        "\n",
        "  for image in ori_images:\n",
        "    inference_state = processor.set_image(image)\n",
        "    # Prompt the model with text\n",
        "\n",
        "    def overlay_masks(image, masks, colors, transparency):\n",
        "        \"\"\"\n",
        "        masks: (N, H, W)\n",
        "        colors: list of (R, G, B)\n",
        "        alpha: 0â€“255\n",
        "        \"\"\"\n",
        "        image = image.convert(\"RGBA\")\n",
        "\n",
        "        if masks.ndim == 4:\n",
        "            masks = masks.squeeze(0)\n",
        "\n",
        "        masks = masks.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        for mask, color in zip(masks, colors):\n",
        "            mask_img = Image.fromarray(mask * 255, mode=\"L\")\n",
        "            overlay = Image.new(\"RGBA\", image.size, color + (0,))\n",
        "            overlay.putalpha(mask_img.point(lambda v: transparency if v > 0 else 0))\n",
        "            image = Image.alpha_composite(image, overlay)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "    def best_mask_from_prompt(processor, inference_state, prompt):\n",
        "        with torch.inference_mode():\n",
        "            output = processor.set_text_prompt(\n",
        "                state=inference_state,\n",
        "                prompt=prompt,\n",
        "            )\n",
        "\n",
        "        masks, boxes, scores = output[\"masks\"], output[\"boxes\"], output[\"scores\"]\n",
        "\n",
        "        if len(masks) == 0:\n",
        "            # print(f\"No masks found for prompt '{prompt}'\")\n",
        "            return None, None\n",
        "\n",
        "        else:\n",
        "\n",
        "          best_idx = torch.argmax(scores).item()\n",
        "\n",
        "          # normalize to (1, H, W)\n",
        "          if masks.ndim == 4:\n",
        "              masks = masks.squeeze(0)\n",
        "          best_mask = masks[best_idx]\n",
        "\n",
        "          if best_mask.ndim == 2:\n",
        "              best_mask = best_mask.unsqueeze(0)\n",
        "\n",
        "          return best_mask, scores[best_idx].item()\n",
        "\n",
        "    prompts = extract_objects(instruction)\n",
        "    results = []\n",
        "    all_masks = []\n",
        "    all_colors = []\n",
        "    all_scores = []\n",
        "\n",
        "    DEFAULT_COLORS = [\n",
        "      (160, 32, 240),  # purple\n",
        "      (255, 0, 0),     # red\n",
        "      (255, 255, 0),   # yellow\n",
        "      (0, 0, 255),     # blue\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        mask, score = best_mask_from_prompt(\n",
        "            processor,\n",
        "            inference_state,\n",
        "            prompt,\n",
        "        )\n",
        "\n",
        "        if mask is None:\n",
        "            continue\n",
        "\n",
        "        all_masks.append(mask)\n",
        "        all_scores.append(score)\n",
        "\n",
        "        all_colors.append(DEFAULT_COLORS[i % len(DEFAULT_COLORS)])\n",
        "\n",
        "        # print(f\"[{prompt}] score = {score:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "    if len(all_masks) == 0:\n",
        "        masked_images.append(image.convert(\"RGBA\"))\n",
        "    else:\n",
        "        all_masks = torch.cat(all_masks, dim=0)\n",
        "        masked_images.append(\n",
        "            overlay_masks(\n",
        "                image,\n",
        "                all_masks,\n",
        "                colors=all_colors,\n",
        "                transparency=80,\n",
        "            )\n",
        "        )\n",
        "\n",
        "  return masked_images\n",
        "\n",
        "\n",
        "text_labels = extract_objects(instruction)\n",
        "print(\"Instruction: \", instruction)\n",
        "print(\"Objects: \", text_labels)\n",
        "\n",
        "\n",
        "OBSERVATIONS = [\n",
        "    \"exterior_image_1_left\",\n",
        "    \"exterior_image_2_left\",\n",
        "    \"wrist_image_left\",\n",
        "]\n",
        "\n",
        "for episode in ds.shuffle(10, seed=SEED).take(1):\n",
        "\n",
        "    camera_frames = {obs: [] for obs in OBSERVATIONS}\n",
        "\n",
        "    for step in episode[\"steps\"]:\n",
        "        for obs in OBSERVATIONS:\n",
        "            img_np = step[\"observation\"][obs].numpy()\n",
        "            camera_frames[obs].append(Image.fromarray(img_np))\n",
        "\n",
        "    masked_camera_frames = {\n",
        "        obs: main_pipeline(camera_frames[obs], instruction)\n",
        "        for obs in OBSERVATIONS\n",
        "    }\n",
        "\n",
        "    final_frames = []\n",
        "    T = len(masked_camera_frames[OBSERVATIONS[0]])\n",
        "\n",
        "    for t in range(T):\n",
        "        concat = np.concatenate(\n",
        "            [\n",
        "                np.array(masked_camera_frames[obs][t].convert(\"RGB\"))\n",
        "                for obs in OBSERVATIONS\n",
        "            ],\n",
        "            axis=1\n",
        "        )\n",
        "        final_frames.append(Image.fromarray(concat))\n",
        "\n",
        "display.Image(as_gif(final_frames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufhNKW3rFRb_"
      },
      "outputs": [],
      "source": [
        "output_path = \"/content/Videos/Masked_Video_2.mp4\"\n",
        "fps = 15  # adjust if needed\n",
        "video = final_frames\n",
        "# Convert first frame to get size\n",
        "\n",
        "first = video[0]\n",
        "if not isinstance(first, np.ndarray):\n",
        "    first = np.array(first)\n",
        "\n",
        "h, w = first.shape[:2]\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "writer = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "for img in video:\n",
        "    if not isinstance(img, np.ndarray):\n",
        "        img = np.array(img)\n",
        "\n",
        "    # Ensure 3 channels\n",
        "    if img.ndim == 2:\n",
        "        img = np.stack([img]*3, axis=-1)\n",
        "\n",
        "    # OpenCV expects BGR\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    writer.write(img)\n",
        "\n",
        "writer.release()\n",
        "\n",
        "print(f\"Saved video to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Potentially use GroundingDino for more accurate masking (creat boundingbox first then read from the box)**"
      ],
      "metadata": {
        "id": "xG00BOIWORWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLpaSsEJXVz7"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0BhcIRDwm0Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
        "    model_id\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzdI1qgHxA5m"
      },
      "outputs": [],
      "source": [
        "pil_img = Image.fromarray(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKbWV6w1y9iv"
      },
      "outputs": [],
      "source": [
        "\n",
        "text_labels = extract_objects(instruction)\n",
        "\n",
        "inputs = processor(images=pil_img, text=text_labels, return_tensors=\"pt\").to(dino_model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QEETtLfzVk-"
      },
      "outputs": [],
      "source": [
        "results = processor.post_process_grounded_object_detection(\n",
        "    outputs,\n",
        "    inputs.input_ids,\n",
        "    threshold=0.4,\n",
        "    text_threshold=0.3,\n",
        "    target_sizes=[pil_img.size[::-1]]\n",
        ")\n",
        "\n",
        "result = results[0]\n",
        "for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n",
        "    box = [round(x, 2) for x in box.tolist()]\n",
        "    print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fXaq4ZReFts"
      },
      "outputs": [],
      "source": [
        "draw_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def visualize_detections(image, results):\n",
        "    img = image.copy()\n",
        "\n",
        "    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "\n",
        "        cv2.rectangle(\n",
        "            img,\n",
        "            (int(x1), int(y1)),\n",
        "            (int(x2), int(y2)),\n",
        "            (0, 255, 0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "        text = f\"{label} {score:.2f}\"\n",
        "        cv2.putText(\n",
        "            img,\n",
        "            text,\n",
        "            (int(x1), int(y1) - 5),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.35,\n",
        "            (0, 255, 0),\n",
        "            1\n",
        "        )\n",
        "\n",
        "    return img\n",
        "\n",
        "vis = visualize_detections(draw_img, result)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fRGs92wmJnu"
      },
      "outputs": [],
      "source": [
        "# font size for axes titles\n",
        "plt.rcParams[\"axes.titlesize\"] = 12\n",
        "plt.rcParams[\"figure.titlesize\"] = 12\n",
        "\n",
        "\n",
        "def propagate_in_video(predictor, session_id):\n",
        "    # we will just propagate from frame 0 to the end of the video\n",
        "    outputs_per_frame = {}\n",
        "    for response in predictor.handle_stream_request(\n",
        "        request=dict(\n",
        "            type=\"propagate_in_video\",\n",
        "            session_id=session_id,\n",
        "        )\n",
        "    ):\n",
        "        outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n",
        "\n",
        "    return outputs_per_frame\n",
        "\n",
        "\n",
        "def abs_to_rel_coords(coords, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"):\n",
        "    \"\"\"Convert absolute coordinates to relative coordinates (0-1 range)\n",
        "\n",
        "    Args:\n",
        "        coords: List of coordinates\n",
        "        coord_type: 'point' for [x, y] or 'box' for [x, y, w, h]\n",
        "    \"\"\"\n",
        "    if coord_type == \"point\":\n",
        "        return [[x / IMG_WIDTH, y / IMG_HEIGHT] for x, y in coords]\n",
        "    elif coord_type == \"box\":\n",
        "        return [\n",
        "            [x / IMG_WIDTH, y / IMG_HEIGHT, w / IMG_WIDTH, h / IMG_HEIGHT]\n",
        "            for x, y, w, h in coords\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown coord_type: {coord_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_u0YyVpzb7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ThYq5HLaukm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "gpus_to_use = range(torch.cuda.device_count())\n",
        "\n",
        "predictor = build_sam3_video_predictor(\n",
        "    gpus_to_use=gpus_to_use\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5cWC8MBdVGS"
      },
      "outputs": [],
      "source": [
        "print(os.path.exists(\"/content/droid_frames_exterior_2_left\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJA0dnducjJ6"
      },
      "outputs": [],
      "source": [
        "response = predictor.handle_request(\n",
        "    request=dict(\n",
        "        type=\"start_session\",\n",
        "        resource_path=\"/content/droid_frames_exterior_2_left\",\n",
        "    )\n",
        ")\n",
        "\n",
        "session_id = response[\"session_id\"]\n",
        "print(\"Session ID:\", session_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPoyIwt-mZ7q"
      },
      "outputs": [],
      "source": [
        "video_path = \"/content/droid_frames_exterior_2_left\"\n",
        "\n",
        "if isinstance(video_path, str) and video_path.endswith(\".mp4\"):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_frames_for_vis = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    cap.release()\n",
        "else:\n",
        "    video_frames_for_vis = glob.glob(os.path.join(video_path, \"*.jpg\"))\n",
        "    try:\n",
        "        # integer sort instead of string sort (so that e.g. \"2.jpg\" is before \"11.jpg\")\n",
        "        video_frames_for_vis.sort(\n",
        "            key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n",
        "        )\n",
        "    except ValueError:\n",
        "        # fallback to lexicographic sort if the format is not \"<frame_index>.jpg\"\n",
        "        print(\n",
        "            f'frame names are not in \"<frame_index>.jpg\" format: {video_frames_for_vis[:5]=}, '\n",
        "            f\"falling back to lexicographic sort.\"\n",
        "        )\n",
        "        video_frames_for_vis.sort()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDqj3H5kd-QF"
      },
      "outputs": [],
      "source": [
        "def dino_to_sam3_box(box, img_w, img_h):\n",
        "    x1, y1, x2, y2 = box\n",
        "    return [\n",
        "        x1 / img_w,\n",
        "        y1 / img_h,\n",
        "        (x2 - x1) / img_w,\n",
        "        (y2 - y1) / img_h,\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqyTozc5iwcH"
      },
      "outputs": [],
      "source": [
        "print(result[\"boxes\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ6ycmrylZ_K"
      },
      "outputs": [],
      "source": [
        "_ = predictor.handle_request(\n",
        "    request=dict(\n",
        "        type=\"reset_session\",\n",
        "        session_id=session_id,\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHGFQrG_eHIt"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH, IMG_HEIGHT = pil_img.size\n",
        "\n",
        "text_prompts=[\"pen\"]\n",
        "objects_ids = [0]\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "for obj_id, text_prompt in enumerate(text_prompts):\n",
        "    response = predictor.handle_request(\n",
        "        request=dict(\n",
        "            type=\"add_prompt\",\n",
        "            session_id=session_id,\n",
        "            frame_index=frame_idx,\n",
        "            text=text_prompt,\n",
        "            obj_id=obj_id,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(f\"Added text prompt '{text_prompt}' as object {obj_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j270Quhflzie"
      },
      "outputs": [],
      "source": [
        "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
        "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
        "\n",
        "vis_frame_stride = 60\n",
        "plt.close(\"all\")\n",
        "\n",
        "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
        "    visualize_formatted_frame_output(\n",
        "        frame_idx,\n",
        "        video_frames_for_vis,\n",
        "        outputs_list=[outputs_per_frame],\n",
        "        titles=[\"SAM 3 Multi-Object Tracking\"],\n",
        "        figsize=(6, 4),\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}