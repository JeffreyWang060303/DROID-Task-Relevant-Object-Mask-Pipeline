# -*- coding: utf-8 -*-
"""Task Relevant Object Mask Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19rmaxP0fCp4w4Tlk8_Xo_3KNb6l6eJHx
"""

!nvidia-smi

from huggingface_hub import login
login(new_session=False)

!pip install decord
!pip install git+https://github.com/facebookresearch/sam3.git

import tensorflow_datasets as tfds
import numpy as np
from PIL import Image
from IPython import display
import IPython.display as ipd
import os
from tqdm import tqdm
import matplotlib
import matplotlib.pyplot as plt
import cv2
import spacy

from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
import torch
from PIL import Image

from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor

import glob
import os

from sam3.visualization_utils import (
    load_frame,
    prepare_masks_for_visualization,
    visualize_formatted_frame_output,
)

import sam3

def as_gif(images, path="temp.gif"):
  # Render the images as the gif (15Hz control frequency):
  images[0].save(path, save_all=True, append_images=images[1:], duration=int(1000/15), loop=0)
  gif_bytes = open(path,"rb").read()
  return gif_bytes

#OPTIONAL: Save the images

OUTPUT_DIR = "/content/droid_frames_exterior_2_left"
os.makedirs(OUTPUT_DIR, exist_ok=True)

ds = tfds.load(
    "droid_100",
    data_dir="gs://gresearch/robotics",
    split="train"
)

frame_idx = 0

for episode in ds.shuffle(10, seed=6).take(1):
    for step in tqdm(episode["steps"]):
        frame = step["observation"]["exterior_image_1_left"].numpy()

        img = Image.fromarray(frame)
        img.save(
            os.path.join(OUTPUT_DIR, f"{frame_idx:05d}.jpg"),
            quality=95,
            subsampling=0
        )

        frame_idx += 1

print(f"Saved {frame_idx} frames to {OUTPUT_DIR}")

"""**Use Spacy for objects extraction from instruction**"""

nlp = spacy.load("en_core_web_sm")

def extract_objects(instruction):
    doc = nlp(instruction.lower())
    objects = []

    for chunk in doc.noun_chunks:

        token_text = chunk.text.strip()

        if token_text in ["a", "the", "it", "they", "them", "this", "that"]:
            continue

        if token_text not in objects:
            objects.append(token_text)

    objects = [obj for obj in objects if obj.isalpha() or " " in obj]

    return objects

i =   "Pick up the blue ring from the table and put it in the wooden tray"
print(extract_objects(i))

"""**Sam3 only Method (Prompt: text)**"""

# Load the model
model = build_sam3_image_model()
processor = Sam3Processor(model)

ds = tfds.load("droid_100", data_dir="gs://gresearch/robotics", split="train")
SEED = 3
# 6

for episode in ds.shuffle(10, seed=SEED).take(1):
  # print(episode)
  steps = episode["steps"]
  first_step = next(iter(steps))
  instruction = first_step["language_instruction"].numpy().decode("utf-8")

original_images = []
for episode in ds.shuffle(10, seed=SEED).take(1):
  for i, step in enumerate(episode["steps"]):
    original_images.append(
      Image.fromarray(
        np.concatenate((
              step["observation"]["exterior_image_1_left"].numpy(),
              step["observation"]["exterior_image_2_left"].numpy(),
              step["observation"]["wrist_image_left"].numpy(),
        ), axis=1)
      )
    )

text_labels = extract_objects(instruction)
print("Instruction: ", instruction)
print("Objects: ", text_labels)
display.Image(as_gif(original_images))

# for episode in ds.shuffle(10, seed=SEED).take(1):
#   # print(episode)
#   steps = episode["steps"]
#   first_step = next(iter(steps))
#   instruction = first_step["language_instruction_2"].numpy().decode("utf-8")
#   print(instruction)

def main_pipeline(ori_images, instr):

  masked_images = []

  for image in ori_images:
    inference_state = processor.set_image(image)
    # Prompt the model with text

    def overlay_masks(image, masks, colors, transparency):
        """
        masks: (N, H, W)
        colors: list of (R, G, B)
        alpha: 0â€“255
        """
        image = image.convert("RGBA")

        if masks.ndim == 4:
            masks = masks.squeeze(0)

        masks = masks.cpu().numpy().astype(np.uint8)

        for mask, color in zip(masks, colors):
            mask_img = Image.fromarray(mask * 255, mode="L")
            overlay = Image.new("RGBA", image.size, color + (0,))
            overlay.putalpha(mask_img.point(lambda v: transparency if v > 0 else 0))
            image = Image.alpha_composite(image, overlay)

        return image


    def best_mask_from_prompt(processor, inference_state, prompt):
        with torch.inference_mode():
            output = processor.set_text_prompt(
                state=inference_state,
                prompt=prompt,
            )

        masks, boxes, scores = output["masks"], output["boxes"], output["scores"]

        if len(masks) == 0:
            # print(f"No masks found for prompt '{prompt}'")
            return None, None

        else:

          best_idx = torch.argmax(scores).item()

          # normalize to (1, H, W)
          if masks.ndim == 4:
              masks = masks.squeeze(0)
          best_mask = masks[best_idx]

          if best_mask.ndim == 2:
              best_mask = best_mask.unsqueeze(0)

          return best_mask, scores[best_idx].item()

    prompts = extract_objects(instruction)
    results = []
    all_masks = []
    all_colors = []
    all_scores = []

    DEFAULT_COLORS = [
      (160, 32, 240),  # purple
      (255, 0, 0),     # red
      (255, 255, 0),   # yellow
      (0, 0, 255),     # blue
    ]

    for i, prompt in enumerate(prompts):
        mask, score = best_mask_from_prompt(
            processor,
            inference_state,
            prompt,
        )

        if mask is None:
            continue

        all_masks.append(mask)
        all_scores.append(score)

        all_colors.append(DEFAULT_COLORS[i % len(DEFAULT_COLORS)])

        # print(f"[{prompt}] score = {score:.3f}")



    if len(all_masks) == 0:
        masked_images.append(image.convert("RGBA"))
    else:
        all_masks = torch.cat(all_masks, dim=0)
        masked_images.append(
            overlay_masks(
                image,
                all_masks,
                colors=all_colors,
                transparency=80,
            )
        )

  return masked_images


text_labels = extract_objects(instruction)
print("Instruction: ", instruction)
print("Objects: ", text_labels)


OBSERVATIONS = [
    "exterior_image_1_left",
    "exterior_image_2_left",
    "wrist_image_left",
]

for episode in ds.shuffle(10, seed=SEED).take(1):

    camera_frames = {obs: [] for obs in OBSERVATIONS}

    for step in episode["steps"]:
        for obs in OBSERVATIONS:
            img_np = step["observation"][obs].numpy()
            camera_frames[obs].append(Image.fromarray(img_np))

    masked_camera_frames = {
        obs: main_pipeline(camera_frames[obs], instruction)
        for obs in OBSERVATIONS
    }

    final_frames = []
    T = len(masked_camera_frames[OBSERVATIONS[0]])

    for t in range(T):
        concat = np.concatenate(
            [
                np.array(masked_camera_frames[obs][t].convert("RGB"))
                for obs in OBSERVATIONS
            ],
            axis=1
        )
        final_frames.append(Image.fromarray(concat))

display.Image(as_gif(final_frames))

#Optional: Save the Video

output_path = "/content/Videos/Original_Video_1.mp4"
fps = 15  # adjust if needed
video = original_images
# Convert first frame to get size

first = video[0]
if not isinstance(first, np.ndarray):
    first = np.array(first)

h, w = first.shape[:2]

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
writer = cv2.VideoWriter(output_path, fourcc, fps, (w, h))

for img in video:
    if not isinstance(img, np.ndarray):
        img = np.array(img)

    # Ensure 3 channels
    if img.ndim == 2:
        img = np.stack([img]*3, axis=-1)

    # OpenCV expects BGR
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    writer.write(img)

writer.release()

print(f"Saved video to {output_path}")